### Variables

# Override name
nameOverride: null

# Cluster name
clusterName: ""

### Flags ###
# Logs
logs:
  enabled: true
# Traces
traces:
  enabled: true
# Metrics
metrics:
  enabled: true
# Events
events:
  enabled: true

### GLOBAL CONFIG ###
# Global config for ease of use to apply to all collector types.
global:

  # New Relic account configuration
  newrelic:
    # Flag to enable global New Relic configuration.
    # -> If it is enabled, the individual New Relic sections for deployment, daemonset
    #    and statefulset will be ignored
    enabled: true
    # OTLP endpoint for all New Relic accounts
    # For US accounts -> https://otlp.nr-data.net
    # For EU accounts -> https://otlp.eu01.nr-data.net
    endpoint: "https://otlp.nr-data.net"
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      # OPS team which is responsible for the cluster and common apps
      # running on it.
      opsteam:
        # New Relic ingest license key
        # -> Use either "value" or "secretRef" where "secretRef" will precede if both are defined
        licenseKey:
          # If you want to create a new secret, provide to the license key as a Helm value.
          value: ""
          # If you already have your license key as a secret stored within the same
          # namespace as this Helm deployment, provide the secret name and the key to
          # license key.
          secretRef: null
            # name: ""
            # key: ""
        # Namespaces to filter the gathered telemetry data
        # -> If nothing is defined, all telemetry data will be sent
        namespaces: []

      # # If you want to send the namespaced telemetry data from the
      # # cluster to the accounts of the individual dev teams
      # # comment in below.
      # # Dev team 1 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam1:
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam1
      # # Dev team 2 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam2:
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam2
      # # Dev team 3 which is responsible for its own apps running
      # # in a specific namespace but not on this cluster.
      # devteam3:
      #   ignore: true
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam3

### DEPLOYMENT CONFIG ###
# This configuration creates 2 collectors as Kubernetes deployments:
# - receiver
# - sampler

## Receiver collector
# It is responsible for gathering the metrics & spans from all of the applications.
# Metrics
# - They will be filtered according to teams and will be exported to each team's
#   account.
# Traces
# - They will be sent to the sampler collector per the loadbalancingexporter
#   according to the trace IDs.

## Sampler collector
# It is responsible to:
# - gather the spans from the receiver collector
# - sample them
# - filter & forward them to corresponding New Relic accounts

# Both collectors can be scaled out/in independently from each other
deployment:

  # Image
  image:
    # Repository
    repository: otel/opentelemetry-collector-contrib
    # Image pull policy
    pullPolicy: IfNotPresent
    # Image tag
    tag: ""

  # Number of replicas
  replicas:
    # Receiver collectors
    receiver: 1
    # Sampler collectors
    sampler: 1

  # Service account
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}

  # Security context for container priviliges
  securityContext: {}

  # Annotations for collector pods
  annotations: {}

  clusterRole:
    # Annotations to add to the clusterRole
    # Can be used in combination with presets that create a cluster role.
    annotations: {}
    # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Can be used in combination with presets that create a cluster role to add additional rules.
    rules:
      - apiGroups:
          - ""
        resources:
          - pods
          - namespaces
        verbs:
          - get
          - watch
          - list

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}

  # Array of key value pairs defining the ports for the
  # collector to expose
  ports:
    # Prometheus
    prometheus:
      name: prometheus
      protocol: TCP
      port: 8888
      targetPort: 8888

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    requests:
      cpu: 32m
      memory: 128Mi
    limits:
      cpu: 256m
      memory: 512Mi

  # Specific Prometheus configuration
  prometheus:
    # Receiver collectors
    receiver:
      # Collector scraping its own metrics
      self:
        # Scrape interval
        scrapeInterval: 60s
        # Keeps only the most important metrics and drops the rest of the scraped metrics
        importantMetrics: []
          # - otelcol_exporter_queue_size
          # - otelcol_exporter_queue_capacity
          # - otelcol_processor_dropped_metric_points
          # - otelcol_processor_dropped_spans
          # - otelcol_processor_dropped_log_records
          # - otelcol_exporter_enqueue_failed_metric_points
          # - otelcol_exporter_enqueue_failed_spans
          # - otelcol_exporter_enqueue_failed_log_records
          # - otelcol_receiver_refused_metric_points
          # - otelcol_receiver_refused_spans
          # - otelcol_receiver_refused_log_records
          # - otelcol_exporter_refused_metric_points
          # - otelcol_exporter_refused_spans
          # - otelcol_exporter_refused_log_records

    # Sampler collectors
    sampler:
      # Collector scraping its own metrics
      self:
        # Scrape interval
        scrapeInterval: 60s
        # Keeps only the most important metrics and drops the rest of the scraped metrics
        importantMetrics: []
          # - otelcol_exporter_queue_size
          # - otelcol_exporter_queue_capacity
          # - otelcol_processor_dropped_metric_points
          # - otelcol_processor_dropped_spans
          # - otelcol_processor_dropped_log_records
          # - otelcol_exporter_enqueue_failed_metric_points
          # - otelcol_exporter_enqueue_failed_spans
          # - otelcol_exporter_enqueue_failed_log_records
          # - otelcol_receiver_refused_metric_points
          # - otelcol_receiver_refused_spans
          # - otelcol_receiver_refused_log_records
          # - otelcol_exporter_refused_metric_points
          # - otelcol_exporter_refused_spans
          # - otelcol_exporter_refused_log_records

  # New Relic account configuration
  # -> If the global New Relic configuration is enabled, this section will be ignored
  newrelic:
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      # OPS team which is responsible for the cluster and common apps
      # running on it.
      opsteam:
        # OTLP endpoint
        # For US accounts -> https://otlp.nr-data.net
        # For EU accounts -> https://otlp.eu01.nr-data.net
        endpoint: "https://otlp.nr-data.net"
        # New Relic ingest license key
        # -> Use either "value" or "secretRef" where "secretRef" will precede if both are defined
        licenseKey:
          # If you want to create a new secret, provide to the license key as a Helm value.
          value: ""
          # If you already have your license key as a secret stored within the same
          # namespace as this Helm deployment, provide the secret name and the key to
          # license key.
          secretRef: null
            # name: ""
            # key: ""
        # Namespaces to filter the gathered telemetry data
        # -> If nothing is defined, all telemetry data will be sent
        namespaces: []

      # # If you want to send the namespaced telemetry data from the
      # # cluster to the accounts of the individual dev teams
      # # comment in below.
      # # Dev team 1 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam1:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam1
      # # Dev team 2 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam2:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam2
      # # Dev team 3 which is responsible for its own apps running
      # # in a specific namespace but not on this cluster.
      # devteam3:
      #   ignore: true
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam3

### DAEMONSET CONFIG ###
# This configuration is responsible for collecting the logs from the
# applications running on the cluster.
daemonset:

  # Image
  image:
    # Repository
    repository: otel/opentelemetry-collector-contrib
    # Image pull policy
    pullPolicy: IfNotPresent
    # Image tag
    tag: ""

  # Service account
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}

  # Security context for container priviliges
  securityContext: {}

  # Annotations for collector pods
  annotations: {}

  clusterRole:
    # Annotations to add to the clusterRole
    # Can be used in combination with presets that create a cluster role.
    annotations: {}
    # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Can be used in combination with presets that create a cluster role to add additional rules.
    rules:
      - apiGroups:
          - ""
        resources:
          - pods
          - namespaces
        verbs:
          - get
          - watch
          - list

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}

  # Array of key value pairs defining the ports for the
  # collector to expose
  ports:
    # Prometheus
    prometheus:
      name: prometheus
      protocol: TCP
      port: 8888
      targetPort: 8888

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    requests:
      cpu: 32m
      memory: 128Mi
    limits:
      cpu: 256m
      memory: 512Mi

  # Specific Prometheus configuration
  prometheus:
    # Collector scraping its own metrics
    self:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - otelcol_exporter_queue_size
        # - otelcol_exporter_queue_capacity
        # - otelcol_processor_dropped_metric_points
        # - otelcol_processor_dropped_spans
        # - otelcol_processor_dropped_log_records
        # - otelcol_exporter_enqueue_failed_metric_points
        # - otelcol_exporter_enqueue_failed_spans
        # - otelcol_exporter_enqueue_failed_log_records
        # - otelcol_receiver_refused_metric_points
        # - otelcol_receiver_refused_spans
        # - otelcol_receiver_refused_log_records
        # - otelcol_exporter_refused_metric_points
        # - otelcol_exporter_refused_spans
        # - otelcol_exporter_refused_log_records

  # Log tailing & parsing rules
  filelog:
    start_at: end
    include:
    - /var/log/pods/*/*/*.log
    include_file_name: false
    include_file_path: true
    operators:
    - id: get-format
      routes:
      - expr: body matches "^\\{"
        output: parser-docker
      - expr: body matches "^[^ Z]+ "
        output: parser-crio
      - expr: body matches "^[^ Z]+Z"
        output: parser-containerd
      type: router
    - id: parser-crio
      output: extract_metadata_from_filepath
      regex: ^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
      timestamp:
        layout: "2006-01-02T15:04:05.000000000-07:00"
        layout_type: gotime
        parse_from: attributes.time
      type: regex_parser
    - id: parser-containerd
      output: extract_metadata_from_filepath
      regex: ^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$
      timestamp:
        layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        parse_from: attributes.time
      type: regex_parser
    - id: parser-docker
      output: extract_metadata_from_filepath
      timestamp:
        layout: '%Y-%m-%dT%H:%M:%S.%LZ'
        parse_from: attributes.time
      type: json_parser
    - id: extract_metadata_from_filepath
      parse_from: attributes["log.file.path"]
      regex: ^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]+)\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$
      type: regex_parser
    - from: attributes.stream
      to: attributes["log.iostream"]
      type: move
    - from: attributes.container_name
      to: resource["k8s.container.name"]
      type: move
    - from: attributes.namespace
      to: resource["k8s.namespace.name"]
      type: move
    - from: attributes.pod_name
      to: resource["k8s.pod.name"]
      type: move
    - from: attributes.restart_count
      to: resource["k8s.container.restart_count"]
      type: move
    - from: attributes.uid
      to: resource["k8s.pod.uid"]
      type: move
    - from: attributes.log
      to: body
      type: move

  # New Relic account configuration
  # -> If the global New Relic configuration is enabled, this section will be ignored
  newrelic:
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      # OPS team which is responsible for the cluster and common apps
      # running on it.
      opsteam:
        # OTLP endpoint
        # For US accounts -> https://otlp.nr-data.net
        # For EU accounts -> https://otlp.eu01.nr-data.net
        endpoint: "https://otlp.nr-data.net"
        # New Relic ingest license key
        # -> Use either "value" or "secretRef" where "secretRef" will precede if both are defined
        licenseKey:
          # If you want to create a new secret, provide to the license key as a Helm value.
          value: ""
          # If you already have your license key as a secret stored within the same
          # namespace as this Helm deployment, provide the secret name and the key to
          # license key.
          secretRef: null
            # name: ""
            # key: ""
        # Namespaces to filter the gathered telemetry data
        # -> If nothing is defined, all telemetry data will be sent
        namespaces: []

      # # If you want to send the namespaced telemetry data from the
      # # cluster to the accounts of the individual dev teams
      # # comment in below.
      # # Dev team 1 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam1:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam1
      # # Dev team 2 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam2:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam2
      # # Dev team 3 which is responsible for its own apps running
      # # in a specific namespace but not on this cluster.
      # devteam3:
      #   ignore: true
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam3

### STATEFULSET CONFIG ###
# This configuration is responsible for scraping the metrics from the
# various endpoints within the cluster.
statefulset:

  # Image
  image:
    # Repository
    repository: otel/opentelemetry-collector-contrib
    # Image pull policy
    pullPolicy: IfNotPresent
    # Image tag
    tag: ""

  # Number of replicas
  replicas: 2

  # Service account
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}

  # Security context for container priviliges
  securityContext: {}

  # Annotations for collector pods
  annotations: {}

  clusterRole:
    # Annotations to add to the clusterRole
    # Can be used in combination with presets that create a cluster role.
    annotations: {}
    # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Can be used in combination with presets that create a cluster role to add additional rules.
    rules:
      - apiGroups:
        - ""
        resources:
          - events
          - namespaces
          - namespaces/status
          - nodes
          - nodes/spec
          - nodes/stats
          - nodes/proxy
          - nodes/metrics
          - pods
          - pods/status
          - replicationcontrollers
          - replicationcontrollers/status
          - resourcequotas
          - services
          - endpoints
          - ingresses
          - configmaps
        verbs:
          - get
          - list
          - watch
      - apiGroups:
        - apps
        resources:
          - daemonsets
          - deployments
          - replicasets
          - statefulsets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - extensions
        resources:
          - daemonsets
          - deployments
          - replicasets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - batch
        resources:
          - jobs
          - cronjobs
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - autoscaling
        resources:
          - horizontalpodautoscalers
        verbs:
          - get
          - list
          - watch
      - nonResourceURLs:
          - "/metrics"
          - "/metrics/cadvisor"
        verbs:
          - "get"

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}

  # Array of key value pairs defining the ports for the
  # collector to expose
  ports:
    # Prometheus
    prometheus:
      name: prometheus
      protocol: TCP
      port: 8888
      targetPort: 8888

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    requests:
      cpu: 32m
      memory: 128Mi
    limits:
      cpu: 256m
      memory: 512Mi

  # Target allocator
  targetAllocator:
    # Strategy to filter the discovered targets before distributing them across collectors.
    # Options:
    # - relabel-config (default)
    filterStrategy: relabel-config
    # Strategy to distribute targets across collectors.
    # Options:
    # - consistent-hashing (default)
    # - least-weighted
    allocationStrategy: consistent-hashing

  # Specific Prometheus configuration
  prometheus:
    # Collector scraping its own metrics
    self:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - otelcol_exporter_queue_size
        # - otelcol_exporter_queue_capacity
        # - otelcol_processor_dropped_metric_points
        # - otelcol_processor_dropped_spans
        # - otelcol_processor_dropped_log_records
        # - otelcol_exporter_enqueue_failed_metric_points
        # - otelcol_exporter_enqueue_failed_spans
        # - otelcol_exporter_enqueue_failed_log_records
        # - otelcol_receiver_refused_metric_points
        # - otelcol_receiver_refused_spans
        # - otelcol_receiver_refused_log_records
        # - otelcol_exporter_refused_metric_points
        # - otelcol_exporter_refused_spans
        # - otelcol_exporter_refused_log_records

    # Kube API server specific configuration
    kubeApiServer:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - apiserver_request_duration_seconds_.*
        # - apiserver_request_total
        # - workqueue_adds_total
        # - workqueue_depth
        # - apiserver_current_inflight_requests
        # - apiserver_dropped_requests_total

    # Nodes specific configuration
    nodes:
      # Scrape interval
      scrapeInterval: 60s

    # cAdvisor specific configuration
    cAdvisor:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - container_cpu_usage_seconds_total
        # - container_memory_usage_bytes
        # - container_fs_reads_total
        # - container_fs_writes_total
        # - container_network_receive_bytes_total
        # - container_network_transmit_bytes_total

    # Core DNS specific configuration
    coreDns:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - coredns_dns_request_duration_seconds
        # - coredns_dns_requests_total
        # - coredns_dns_responses_total
        # - coredns_panics_total
        # - coredns_cache_hits_total

    # Node exporter specific configuration
    nodeExporter:
      # - If you don't have any node-exporter, you can let this chart deploy it as a dependency by setting
      #   the field "enabled" to true. You can override its values in the DEPENDENCY CONFIG section of this file.
      # - If you already have a node-exporter on your cluster, you can set the field "enabled" to false and give
      #   its service name as a reference to the field "serviceNameRef". The collector will scrape that service and
      #   will not deploy any node-exporter.
      enabled: true
      serviceNameRef: null
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - node_cpu_seconds_total
        # - node_memory_MemTotal_bytes
        # - node_memory_MemFree_bytes
        # - node_memory_Cached_bytes
        # - node_memory_Buffers_bytes
        # - node_filesystem_avail_bytes
        # - node_filesystem_size_bytes

    # Kube state metrics specific configuration
    kubeStateMetrics:
      # - If you don't have any kube-state-metrics, you can let this chart deploy it as a dependency by setting
      #   the field "enabled" to true. You can override its values in the DEPENDENCY CONFIG section of this file.
      # - If you already have a kube-state-metrics on your cluster, you can set the field "enabled" to false and give
      #   its service name as a reference to the field "serviceNameRef". The collector will scrape that service and
      #   will not deploy any kube-state-metrics.
      enabled: true
      serviceNameRef: null
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - kube_pod_info
        # - kube_pod_status_phase
        # - kube_pod_container_status_waiting
        # - kube_pod_status_scheduled_time
        # - kube_daemonset_created
        # - kube_daemonset_status_number_available
        # - kube_deployment_status_replicas_unavailable
        # - kube_deployment_status_condition
        # - kube_replicaset_status_ready_replicas
        # - kube_replicaset_status_replicas
        # - kube_statefulset_replicas
        # - kube_statefulset_status_replicas
        # - kube_pod_start_time
        # - kube_pod_status_reason
        # - kube_pod_container_resource_limits
        # - kube_pod_status_scheduled
        # - kube_pod_container_resource_requests
        # - kube_pod_status_container_ready_time
        # - kube_pod_container_status_terminated
        # - kube_pod_container_status_ready
        # - kube_statefulset_status_replicas_updated
        # - kube_statefulset_status_replicas_ready
        # - kube_daemonset_status_desired_number_scheduled
        # - kube_deployment_status_replicas_ready
        # - kube_pod_container_state_started
        # - kube_statefulset_created
        # - kube_daemonset_status_updated_number_scheduled
        # - kube_statefulset_status_replicas_current
        # - kube_daemonset_status_number_ready
        # - kube_pod_container_status_running
        # - kube_pod_created
        # - kube_pod_container_status_restarts_total
        # - kube_daemonset_status_number_unavailable

    # Kubernetes service endpoint specific configuration
    serviceEndpoints:
      # Flag whether scrape job should be created
      scrape: true
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - metric1
        # - metric2
        # - metric3

    # Target allocator specific configuration
    targetAllocator:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - opentelemetry_allocator_collectors_allocatable
        # - opentelemetry_allocator_targets
        # - opentelemetry_allocator_targets_per_collector
        # - opentelemetry_allocator_targets_remaining

    # Config for defining extra custom scrape jobs
    extraScrapeJobs:
      # Kubernetes pod specific scrape job
      - job_name: 'kubernetes-pods'
        scrape_interval: 60s
        honor_labels: true

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
            action: drop
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $$1:$$2
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_node_name]
            action: replace
            target_label: node

  # New Relic account configuration
  # -> If the global New Relic configuration is enabled, this section will be ignored
  newrelic:
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      # OPS team which is responsible for the cluster and common apps
      # running on it.
      opsteam:
        # OTLP endpoint
        # For US accounts -> https://otlp.nr-data.net
        # For EU accounts -> https://otlp.eu01.nr-data.net
        endpoint: "https://otlp.nr-data.net"
        # New Relic ingest license key
        # -> Use either "value" or "secretRef" where "secretRef" will precede if both are defined
        licenseKey:
          # If you want to create a new secret, provide to the license key as a Helm value.
          value: ""
          # If you already have your license key as a secret stored within the same
          # namespace as this Helm deployment, provide the secret name and the key to
          # license key.
          secretRef: null
            # name: ""
            # key: ""
        # Namespaces to filter the gathered telemetry data
        # -> If nothing is defined, all telemetry data will be sent
        namespaces: []

      # # If you want to send the namespaced telemetry data from the
      # # cluster to the accounts of the individual dev teams
      # # comment in below.
      # # Dev team 1 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam1:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam1
      # # Dev team 2 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam2:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam2
      # # Dev team 3 which is responsible for its own apps running
      # # in a specific namespace but not on this cluster.
      # devteam3:
      #   ignore: true
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam3

### SINGLETON CONFIG ###
# This configuration is responsible for scraping the events from the
# Kube-API-Server.
singleton:

  # Image
  image:
    # Repository
    repository: otel/opentelemetry-collector-contrib
    # Image pull policy
    pullPolicy: IfNotPresent
    # Image tag
    tag: ""

  # Service account
  serviceAccount:
    # Annotations to add to the service account
    annotations: {}

  # Security context for container priviliges
  securityContext: {}

  # Annotations for collector pods
  annotations: {}

  clusterRole:
    # Annotations to add to the clusterRole
    # Can be used in combination with presets that create a cluster role.
    annotations: {}
    # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
    # Can be used in combination with presets that create a cluster role to add additional rules.
    rules:
      - apiGroups:
          - ""
        resources:
          - events
          - namespaces
          - namespaces/status
          - nodes
          - nodes/spec
          - pods
          - pods/status
          - replicationcontrollers
          - replicationcontrollers/status
          - resourcequotas
          - services
        verbs:
          - get
          - list
          - watch
      - apiGroups:
        - apps
        resources:
          - daemonsets
          - deployments
          - replicasets
          - statefulsets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - extensions
        resources:
          - daemonsets
          - deployments
          - replicasets
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - batch
        resources:
          - jobs
          - cronjobs
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - autoscaling
        resources:
          - horizontalpodautoscalers
        verbs:
          - get
          - list
          - watch

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}

  # Array of key value pairs defining the ports for the
  # collector to expose
  ports:
    # Prometheus
    prometheus:
      name: prometheus
      protocol: TCP
      port: 8888
      targetPort: 8888

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    requests:
      cpu: 32m
      memory: 128Mi
    limits:
      cpu: 256m
      memory: 512Mi

  # Specific Prometheus configuration
  prometheus:
    # Collector scraping its own metrics
    self:
      # Scrape interval
      scrapeInterval: 60s
      # Keeps only the most important metrics and drops the rest of the scraped metrics
      importantMetrics: []
        # - otelcol_exporter_queue_size
        # - otelcol_exporter_queue_capacity
        # - otelcol_processor_dropped_metric_points
        # - otelcol_processor_dropped_spans
        # - otelcol_processor_dropped_log_records
        # - otelcol_exporter_enqueue_failed_metric_points
        # - otelcol_exporter_enqueue_failed_spans
        # - otelcol_exporter_enqueue_failed_log_records
        # - otelcol_receiver_refused_metric_points
        # - otelcol_receiver_refused_spans
        # - otelcol_receiver_refused_log_records
        # - otelcol_exporter_refused_metric_points
        # - otelcol_exporter_refused_spans
        # - otelcol_exporter_refused_log_records

  # New Relic account configuration
  # -> If the global New Relic configuration is enabled, this section will be ignored
  newrelic:
    # Teams to segragete the telemetry data received by all of the collectors.
    teams:
      # OPS team which is responsible for the cluster and common apps
      # running on it.
      opsteam:
        # OTLP endpoint
        # For US accounts -> https://otlp.nr-data.net
        # For EU accounts -> https://otlp.eu01.nr-data.net
        endpoint: "https://otlp.nr-data.net"
        # New Relic ingest license key
        # -> Use either "value" or "secretRef" where "secretRef" will precede if both are defined
        licenseKey:
          # If you want to create a new secret, provide to the license key as a Helm value.
          value: ""
          # If you already have your license key as a secret stored within the same
          # namespace as this Helm deployment, provide the secret name and the key to
          # license key.
          secretRef: null
            # name: ""
            # key: ""
        ###################################################################
        ### Namespaced filtering does not work for events at the moment ###
        ###################################################################
        # Namespaces to filter the gathered telemetry data
        # -> If nothing is defined, all telemetry data will be sent
        # namespaces: []

      ###################################################################
      ### Multi-account export does not work for events at the moment ###
      ###################################################################
      # # If you want to send the namespaced telemetry data from the
      # # cluster to the accounts of the individual dev teams
      # # comment in below.
      # # Dev team 1 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam1:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam1
      # # Dev team 2 which is responsible for its own apps running
      # # in a specific namespace.
      # devteam2:
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam2
      # # Dev team 3 which is responsible for its own apps running
      # # in a specific namespace but not on this cluster.
      # devteam3:
      #   ignore: true
      #   endpoint: "https://otlp.nr-data.net"
      #   licenseKey:
      #     value: ""
      #   namespaces:
      #     - devteam3

### DEPENDENCY CONFIG ###
# You can override default values for the dependency helm charts in this section

# Node exporter
prometheus-node-exporter:
  # Tolerations override
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

# Kube state metrics
kube-state-metrics:
  # Auto-sharding for horizontal scalibility
  autosharding:
    enabled: true
